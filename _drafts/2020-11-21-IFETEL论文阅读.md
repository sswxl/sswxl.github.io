---
layout: post
title: Improving Fine-grained Entity Typing with Entity Linking 
subtitle：论文阅读
date: 2020-11-21
---

# Improving Fine-grained Entity Typing with Entity Linking  

#　Abstract  

细粒度的实体类型输入是一个具有挑战性的问题，因为它通常涉及相对较大的标记集，并且可能需要了解实体提及的上下文。在本文中，我们使用实体链接来帮助进行细粒度的实体类型分类处理。我们
提出了一个深层神经模型，该模型基于**上下文**和**从实体链接结果中获得的信息**进行预测。在两个常用数据集上的实验结果证明了我们方法的有效性。在这两个数据集上，它都比现有技术实现了超过5％的绝对严格精度改进。

# Introduction  　　

给定一段文字和本文中提及实体的范围，细粒度实体类型分类（FET）是为提及内容分配细粒度类型标签的任务。分配的标签标签应**依赖于上下文**例如，在“特朗普威胁要把美国从世界贸易组织中撤出。”
虽然“唐纳德·特朗普（Donald Trump）也有其他职业，例如商务人士，电视名人等，但提到的“特朗普”应标为/person和/person/politician 。

该任务具有挑战性，因为它通常使用相对较**大的标签集**，并且有些提及可能需**要正确理解上下文**的标签。此外，由于人工注释的工作量很大，因此现有方法必须依靠远程监督来训练模型。

因此，**使用额外的信息**来帮助分类过程变得非常重要。在本文中，我们通过实体链接（EL）改进了FET 。EL对于模型做出分类决定很有帮助，因为如果提及正确链接到它的目标实体，我们可以直接
在知识库（KB）中获取有关该实体的类型信息。例如，在句子“关于Federal Way  的各种问题上进行了很多讨论”中，某些FET模型可能将“Federal Way  ”的提法错误地标记为公司。将其链接到华盛顿的Federal Way 市后，可以避免这种错误。对于需要了解上下文的情况，使用实体链接结果也是有益的。在上述提到“特朗普”的示例中，获得了知识库中所有类型的唐纳德·特朗普（例如，政治家，
人，电视人物等）仍然可以提供信息，以推断出适合背景的正确类型（即政治人物），因为它们会**缩小可能的标签范围**。但是，通过EL获得的信息不应完全信任，因为它**并不总是准确**的。即使提及正确链接到实体，KB中该实体的类型信息**也可能不完整或过时**。因此，在本文中，我们提出了一种深层神经细粒度实体类型输入模型，该模型可以根据**上下文，提及字符串**以及使用**EL获得的KB中的类型信息**灵活地预测标签。

使用EL还为训练程带来了新问题。 当前，构建FET训练样本的一种广泛使用的方法是使用Wikipedia中的**锚点链接**(Anchor Links)。每个锚链接均被视为提及，并以KB所指向实体（锚链指向的Wikipedia页面）的所有类型进行弱标记。当正确链接提及内容时，我们的方法还使用KB中所指向实体的所有类型作为额外信息。这可能会导致训练后的模型**过拟合弱标签数据**。我们设计了合页损失（hinge loss  ）的变体，并在训练过程中引入了噪音以解决此问题。


6211

我们通常在两个方面进行实验

使用的FET数据集。实验结果表明，
引入通过实体
链接获得的信息并具有深度神经模型都有助于
改善FET性能。我们的模型在两个数据集上都比现有技术的
绝对严格精度提高
了5％以上。

我们的贡献总结如下：

•我们提出了一个深层的神经细粒度实体

类型模型，该模型利用
通过实体链接获得的KB类型信息。

•我们解决了我们的模型可能会遇到的问题

通过使用
铰链损耗的变体并
在训练过程中引入噪声，可以使弱标记数据过拟合。

•我们证明我们的应用程序的有效性

在
常用的FET数据集上提供实验结果的方法。

我们的代码位于

[https：// github。](https://github.com/HKUST-KnowComp/IFETEL)

[com / HKUST-KnowComp / IFETEL](https://github.com/HKUST-KnowComp/IFETEL)

[。](https://github.com/HKUST-KnowComp/IFETEL)

2

相关工作


可以在[（）中](#page6-div)找到将命名实体分类为细粒度类型 的早期尝试。

[弗莱施曼](#page6-div)

[和霍维](#page6-div)

[，](#page6-div)

[2002年](#page6-div)

[）， ](#page6-div)仅关注于

儿子的名字。后来，
构造了 具有更大类型集的数据集[（](#page6-div)

[Weischedel和布伦斯坦](#page6-div)

[，](#page6-div)

[2005年](#page6-div)

[;](#page6-div)

[灵与焊](#page6-div)

[，](#page6-div)

[2012年](#page6-div)

[;](#page6-div)

[崔等。](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[）。 ](#page6-div)这些

最近的研究更倾向于使用数据集 [（](#page6-div)

[仁](#page6-div)

[等。](#page6-div)

[，](#page6-div)

[2016年](#page6-div)

[;](#page6-div)

[Murty等。](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[）。](#page6-div)

提议的大多数现有方法

FET是基于学习的。

使用的功能

这些方法可以手工制作 [（](#page6-div)

[凌](#page6-div)

[和焊接](#page6-div)

[，](#page6-div)

[2012年](#page6-div)

[;](#page6-div)

[Gillick等。](#page6-div)

[，](#page6-div)

[2014年](#page6-div)

[） ](#page6-div)或学到的

来自神经网络模型 [（](#page6-div)

[Shimaoka等。](#page6-div)

[，](#page6-div)

[2017年](#page6-div)

[;](#page6-div)

[徐和巴博萨](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[;](#page6-div)

[Xin等。](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[）。](#page6-div)

由于FET系统通常使用远距离监督
进行训练，因此训练样本的标签
可能是嘈杂的，错误的或过于具体的。几项
研究 [（](#page6-div)

[任等人。](#page6-div)

[，](#page6-div)

[2016年](#page6-div)

[;](#page6-div)

[Xin等。](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[;](#page6-div)

[徐和](#page6-div)

[巴博萨](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[） ](#page6-div)通过单独解决这些问题

对干净提及和嘈杂提及进行评分，对
类型进行修正 [（](#page6-div)

[任等人。](#page6-div)

[，](#page6-div)

[2016年](#page6-div)

[）， ](#page6-div)使用

层次结构感知损失 [（](#page6-div)

[徐和巴博萨](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[） ](#page6-div)等

[（](#page6-div)

[黄等。](#page6-div)

[，](#page6-div)

[2016年](#page6-div)

[） ](#page6-div)和 [（](#page6-div)

[周等。](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[）](#page6-div)

与本文最相关的两项研究。

[黄等。](#page6-div)

[（](#page6-div)

[2016年](#page6-div)

[） ](#page6-div)提出无监督的FET

EL是重要组件的系统。但是
他们使用EL来帮助进行聚类和类型名称

选择，这与我们使用
它来改善监督型FET
模型的性能有很大不同。 [（](#page6-div)

[周等。](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[） ](#page6-div)找到相关实体

基于上下文，而不是直接应用
EL。这些实体的类型然后用于
推断提及的类型。

3

方法

令T为预定义标签集，其中包括
我们要分配给提及的所有类型。给定
提及m及其上下文，任务是预测
一组适合于此提及的τ⊂T类型。
因此，这是一个多类别，多标签的分类
问题 [（](#page6-div)

[灵与焊](#page6-div)

[，](#page6-div)

[2012年](#page6-div)

[）。 ](#page6-div)接下来，我们

将详细介绍我们针对此问题的方法
，包括神经模型，模型训练
以及我们使用的实体链接算法。

3.1

细粒度实体键入模型

输入值

FET系统的每个输入样本

包含一个提及及其所属的句子
。我们表示w

1个

，w

2

，...，w

ñ

如在

当前句子，w

p

1个

，w

p

2

，...，w

p

升

如话

在提及字符串中，其中n是
句子中的单词数，p

1个

，...，p

升

是的指数

提及字符串中的单词，l是提及字符串中的
单词数。我们还使用了一组
预训练的单词嵌入。

我们的FET方法如图所示

[1个](#page3-div)

[。](#page3-div)

首先构造三个表示形式：上下文
表示形式

，提及字符串表示形式，以及

KB类型表示

。请注意，KB类型

表示是
通过实体链接从知识库中获得的，并且
与提及内容无关。

上下文表示

获取上下文

表示形式，我们首先使用特殊令牌w

米

至

表示提（标记“[记载]”在
图

[1个](#page3-div)

[）。 ](#page3-div)然后，感官的词序

时态变成w

1个

，...，w

p

升

-1

，w

米

，w

p

升

+1

，...，w

ñ

。

它们对应的词嵌入被馈送到
BiLSTM的两层中。让h

1个

米

和h

2

米

成为


w的BiL- STM第一和第二层的输出

米

， 分别。我们用f

C

=小时

1个

米

+

H

2

米

作为上下文表示向量。

提及字符串表示

让x

1个

， ...， X

升

是提及字符串
词w的词嵌入

p

1个

，...，w

p

升

。然后提及字符串rep-

怨恨f

s

=（

P

升
I = 1

X

一世

）/ l。



![背景图](target003.png)

6212

较早

上

/人/政客

星期二

，

[提及]承诺

至

。

...

...

唐纳德·特朗普

...

...

/ person / tv_personality
/ person / business

从KB类型

**唐纳德·特朗普**（**Donald Trump）**：

一种热编码

类型分数

三层MLP

连接

类型嵌入

/人

/人/演员

/位置/城市

...

双线性STM

平均

EL分数

图1：我们的方法。例句的示例是“周二早些时候，唐纳德·特朗普承诺帮助受重创
的贸易战中受重创的美国农民。” 在这里，提到唐纳德·
特朗普的正确标签

应该是/ person，/ person / politician。“ [提及]”是我们用来表示提及的特殊标记。

KB类型表示

获取知识库

类型表示形式，我们针对
当前提及的内容运行EL算法。如果EL算法返回一个
实体，我们将从
KB中检索该实体的类型。我们使用Freebase作为 [知识库](#page3-div)

[1个](#page3-div)

[。 ](#page3-div)自从

Freebase中的
类型与目标类型集T不同，它们使用
类似于 [（](#page6-div)

[周等。](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[）。](#page6-div)

然后，我们对这些
类型执行一次热编码，以获取KB类型表示形式f

Ë

。如果

EL算法返回NIL（即，提及内容不能
链接到实体），我们只需对
空类型集进行一次热编码即可。

预测

除了这三种表示，

我们还将获得由我们的实体
链接算法返回的分数，这表明它对
链接结果的置信度。我们将其表示为一维
向量g。然后，我们得到f = f

C

⊕f

s

⊕f

Ë

⊕g

⊕表示串联。然后将f馈入
包含三个密集层的MLP中以获得
u

米

，为目前的男性提供最终的代表权-

样品m。让t

1个

t

2

，...，t

ķ

是所有类型

T，其中k = | T |。我们将它们嵌入到与您相同的
空间中

米

通过给他们每个人分配一个密集的

向量 [（](#page6-div)

[Yogatama等。](#page6-div)

[，](#page6-div)

[2015年](#page6-div)

[）](#page6-div)。这些向量是

表示为t

1个

，...，t

ķ

。然后男人的分数-

类型t的m

一世

∈T计算为

你的点积

米

和T

一世

：

s（米，吨

一世

）=你

米

·t

一世

。

（1）

我们预测

一世

作为m的类型，如果s（m，t

一世

）> 0。

1个

我们之所以使用Freebase，主要是因为它已经被ex-

浪费研究。Wikidata是替代方法。

3.2

模型训练

根据现有研究，我们还
通过使用Wikipedia中的锚点链接来生成训练数据。
每个锚链接都可以用作提及。这些
提及通过将
目标条目的Freebase类型映射到标签集T [（](#page6-div)

[凌](#page6-div)

[和焊接](#page6-div)

[，](#page6-div)

[2012年](#page6-div)

[）。](#page6-div)

由于我们使用KB类型表示

我们的FET模型也是通过映射
Freebase类型获得的，它们将完全匹配
自动生成的标签
，以正确链接提及的内容（即，当
由EL算法返回的实体和
锚定链接的目标条目为相同）。例如，
在图

[1个](#page3-div)

[， ](#page3-div)假设例句是一个

从Wikipedia获得的培训样本，其中
“ Donald Trump”是指向
Donald Trump Wikipedia页面的锚链接。

后图

如果将Donald Trump的Freebase类型与
目标标签集联系起来，则该示例将被弱标记
为/ person / politician，/ person / tv个性
和/ person / business，这
与类型信息完全相同（ “类型从KB”，在
图

[1个](#page3-div)

[） ](#page3-div)通过EL获得。因此，在火车上

例如，当EL系统将提及链接到
正确的实体时，模型仅需要以
KB类型表示形式输出类型。这可能会
导致训练后的模型过度拟合弱标签的
训练数据。对于大多数类型的实体，例如
位置和组织，这很好，因为它们
通常在不同的上下文中具有相同的类型。
但这对于个人提及是有问题的，因为他们的
类型可能取决于上下文。

为了解决这个问题，在训练过程中，如果



![背景图](target004.png)

6213

提到通过我们的实体
链接算法链接到一个人实体，我们在生成KB类型表示时添加了一个
不属于该实体的随机细粒度人类型标签
。
例如，如果提及链接到
类型为/ person / actor和/ person / author的
人，则可以添加一个随机标签/ person / politician。即使KB表示形式不再与弱标签完全匹配，这
也将迫使模型仍然
从上下文中推断类型标签。



为了使其更加灵活，我们还建议使用

通过所使用的铰链损失的变体 [（](#page6-div)

[Abhishek等。](#page6-div)

[，](#page6-div)

[2017年](#page6-div)

[） ](#page6-div)来训练我们的模型：

L =

X

米

[

X

t∈τ

米

max（0，1-s（m，t））

+

X

t∈

τ

米

λ（t）max（0，1 + s（m，t））]

（2）

其中τ

米

是提及m的正确类型集，¯

τ

米

是错误的类型集。λ（t）∈[1，+∞）是一个预先
定义的参数，如果
类型t被错误地预测为正，则会施加更大的惩罚。由于
过度标注弱注释
标签的问题对于个人提及更为严重，因此我们将
λ（t）=λ

P

如果t是细粒度的人类型，并且

对于所有其他类型，λ（t）= 1。

在训练期间，我们还随机设置了EL

一半训练样本的结果为NIL。
这样，该模型可以很好地执行
无法在测试时链接到知识库的提及。

3.3

实体链接算法

在本文中，我们使用一种简单的EL算法，
该算法将提及内容直接链接到具有
最大公共性得分的实体。共同点 [（](#page6-div)

[泛](#page6-div)

[等。](#page6-div)

[，](#page6-div)

[2015年](#page6-div)

[;](#page6-div)

[麦德良和莱格](#page6-div)

[，](#page6-div)

[2008年](#page6-div)

[） ](#page6-div)计算

根据Wikipedia中的锚链接确定。估计
仅给出
提及字符串的实体的概率。在我们的FET方法中，
公共分数也被用作对
链接结果的置信度（即，
小节的预测部分中使用的g

[3.1](#page2-div)

[）。 ](#page2-div)在同一文档中，

我们还使用了 [（](#page6-div)

[加尼亚和](#page6-div)

[霍夫曼](#page6-div)

[，](#page6-div)

[2017年](#page6-div)

[） ](#page6-div)查找通用的共同引用

提及人（例如“ Matt”）到更具体的
提及（例如“ Matt Damon”）。

我们还尝试了其他更高级的EL方法

在我们的实验中。但是，它们不能提高
模型的最终性能。
使用[（）中](#page6-div)提出的EL系统的实验结果 

[加尼亚](#page6-div)

[和霍夫曼](#page6-div)

[，](#page6-div)

[2017年](#page6-div)

[） ](#page6-div)在部分中提供

[4](#page4-div)

[。](#page4-div)

4

实验

4.1

建立

我们使用两个数据集：FIGER（GOLD） [（](#page6-div)

[玲和](#page6-div)

[焊接](#page6-div)

[，](#page6-div)

[2012年](#page6-div)

[） ](#page6-div)和BBN [（](#page6-div)

[Weischedel和布伦斯坦](#page6-div)

[，](#page6-div)

[2005年](#page6-div)

[）。 ](#page6-div)它们的标签集的大小分别为113和47，

分别。FIGER（GOLD）允许提及
具有多种类型的路径，但BBN不允许。AN-
等常用数据集，OntoNotes [（](#page6-div)

[吉利克](#page6-div)

[等。](#page6-div)

[，](#page6-div)

[2014年](#page6-div)

[）， ](#page6-div)因为其中包含许多

代词和常用名词短语提及，例如
“ it”，“ he”，“节俭制度”，不
适合直接应用链接的实体。

正在关注 [（](#page6-div)

[灵与焊](#page6-div)

[，](#page6-div)

[2012年](#page6-div)

[）， ](#page6-div)我们生成-

对标记弱的数据集进行评估，以使用
Wikipedia锚链接进行训练。由于
FIGER（GOLD）和BBN使用的标签集不同，因此我们
为它们各自创建一个训练集。对于每个
数据集，随机抽取2 000个弱标记样本
以形成开发集。我们还
手动注释了
从新闻报道中收集的50个人提及，以调整参数λ

P

。

我们使用300维预训练的GloVe

由 [（](#page6-div)

[Pennington等。](#page6-div)

[，](#page6-div)

[2014年](#page6-div)

[）。 ](#page6-div)两层的隐藏层大小

BiLSTM的最大大小都设置为250。对于三层
MLP，两个隐藏层
的大小都设置为500。类型嵌入的大小
是500。λ

P

设置为2.0。我们也采用批处理-


在训练过程中，我们的三层MLP中每个密集层的输入都会出现错误和丢失。

我们使用严格的精度，Macro F1和Micro F1

评估细粒度的打字性能 [（](#page6-div)

[凌](#page6-div)

[和焊接](#page6-div)

[，](#page6-div)

[2012年](#page6-div)

[）。](#page6-div)

4.2

比较方法

我们将其与以下现有方法进行比较
：AFET [（](#page6-div)

[任等人。](#page6-div)

[，](#page6-div)

[2016年](#page6-div)

[）， ](#page6-div) AAA [（](#page6-div)

[Ab-](#page6-div)

[Hishek等。](#page6-div)

[，](#page6-div)

[2017年](#page6-div)

[）， ](#page6-div) NFETC [（](#page6-div)

[徐和巴博萨](#page6-div)

[，](#page6-div)

[2018年](#page6-div)

[） ](#page6-div)和CLSC [（](#page6-div)

[Chen等。](#page6-div)

[，](#page6-div)

[2019年](#page6-div)

[）。](#page6-div)

我们使用我们的（完整）来代表我们的完整模型，

并与我们自己的
方法的五个变体进行比较：我们的（DirectTrain）
在获得
KB类型表示的同时无需添加随机人员类型的情况下进行训练，而λ

P

设置为1；我们的

（NoEL）不使用实体链接，即，删除了KB
类型表示形式和实体链接置信度
分数，并
以DirectTrain样式训练了模型；我们的（NonDeep）使用一层
BiLSTM层，并用致密
层代替了MLP ；Ours（NonDeep NoEL）是Ours（NonDeep）的NoEL版本
。我们的（LocAttEL）使用



![背景图](target005.png)

6214

数据集

格斗（金）

BBN

方法

准确性

宏F1

微型F1

准确性

宏F1

微型F1

场效应管

53.3

69.3

66.4

67.0

72.7

73.5

AAA级

65.8

81.2

77.4

73.3

79.1

79.2

场效应管

68.9

81.9

79.0

72.1

77.1

77.5

CLSC

\--

\--

\--

74.7

80.7

80.5

我们的（NonDeep NoEL）

65.9

81.7

78.0

69.3

81.4

81.5

我们的（NonDeep）

72.3

85.4

82.6

79.1

87.9

88.4

我们的（DirectTrain）

69.1

85.2

82.2

\--

\--

\--

我们的（NoEL）

69.8

82.7

80.4

80.5

87.5

88.0

我们的（LocAttEL）

75.1

86.3

83.9

82.8

88.9

89.5

我们的（完整）

75.5

87.1

84.6

82.5

89.2

89.6

表1：细粒度的实体键入性能。BBN上“ Ours（DirectTrain）”的性能被省略，
因为此数据集没有人的细粒度类型。

[（）中](#page6-div)提出的实体链接方法 

[加尼亚](#page6-div)

[和霍夫曼](#page6-div)

[，](#page6-div)

[2017年](#page6-div)

[）， ](#page6-div)而不是我们自己的共同点-

基于度的方法。我们的（全程），我们的（直达
）

，而我们的（NonDeep）都使用我们自己的com-

基于单体的实体链接方法。

4.3

结果

实验结果列于表

[1个](#page5-div)

[。 ](#page5-div)如

我们可以看到，
在两个数据集上，我们的方法都比现有方法表现更好。

在我们的应用中使用实体链接的好处

可以通过比较Ours（完整）
和Ours（NoEL）来验证方法。
如果删除实体链接部分，则这两个数据集的性能都会降低
。特别是在FIGER（GOLD）上，严格的
准确性从75.5下降到69.8。使用实体
链接对BBN的改善不大。我们认为这是
由于以下三个原因：1）BBN的
标签集比FIGER（GOLD）小得多；2）BBN不允许
使用多种类型的
路径注释注释（例如，
不允许同时使用/ building和/ location标记注释），因此任务比较
容易；3）通过深化模型，
已经大大提高了BBN的性能，这
使得进一步的改进更加困难。

我们全面方法的改进

我们的（DirectTrain）

在FIGER（金）上指示

我们用来避免过度拟合
弱标签数据的技术也很有效。

我们的（LocAttEL）

，它使用了更高级的

EL系统没有
比我们的（Full）使用我们自己的EL
方法更好的性能。在手动检查
两种EL方法的结果以及我们的预测后

在FIGER（GOLD）上建立模型，我们认为这主要是
因为：1）我们的模型
在进行预测时也使用了上下文。有时，如果“认为”
EL提供的类型信息不正确，
则可能不会使用它。2）不同
EL方法的性能还取决于数据集和
用于评估的实体类型。我们发现
在FIGER（GOLD）中，方法 [（](#page6-div)

[加尼亚](#page6-div)

[和霍夫曼](#page6-div)

[，](#page6-div)

[2017年](#page6-div)

[） ](#page6-div)更好地区分lo-

阳离子和运动队，但它也可能会
犯一些我们的简单EL方法没有的错误
。例如，它可能会错误地将月份的“ 3月”链接
到其Wikipedia描述
更加适合上下文的实体。3）
值得一提的是，尽管EL系统将其链接到错误
的实体，但是此实体的类型与
正确的实体相同。

5

结论

我们提出了一种深层神经模型，以
通过实体链接改善细粒度实体的类型。

的


通过使用
铰链损耗的变体并在训练过程中引入噪声，可以解决过度拟合弱标记训练数据的问题。
我们对两个常用
数据集进行实验。实验结果证明了
我们方法的有效性。