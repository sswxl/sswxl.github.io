# Bert

https://www.zhihu.com/column/c_1121788551427215360

1. [BERT](https://github.com/tensorflow/models/blob/master/official/nlp/bert): [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Devlin et al., 2018
2. [ALBERT](https://github.com/tensorflow/models/blob/master/official/nlp/albert): [A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Lan et al., 2019
3. [XLNet](https://github.com/tensorflow/models/blob/master/official/nlp/xlnet): [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Yang et al., 2019
4. [Transformer for translation](https://github.com/tensorflow/models/blob/master/official/nlp/transformer): [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al., 2017
5. [NHNet](https://github.com/tensorflow/models/blob/master/official/nlp/nhnet): [Generating Representative Headlines for News Stories](https://arxiv.org/abs/2001.09386) by Gu et al, 2020