---
layout: post
title: 技术栈
mathjax: true
excerpt_separator: <!--more-->
---

[TOC]





# Papers





## CNN

[LeNet](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)

[VGGNet](https://arxiv.org/pdf/1409.1556.pdf%20http://arxiv.org/abs/1409.1556.pdf)

[Striving for Simplicity: The All Convolutional Net](https://arxiv.org/pdf/1412.6806.pdf%20(http://arxiv.org/pdf/1412.6806.pdf))

[Rethinking the Inception Architecture for Computer Vision](https://openaccess.thecvf.com/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf)

[DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition](http://proceedings.mlr.press/v32/donahue14.pdf)

## RNN

[BRNN](http://deeplearning.cs.cmu.edu/F20/document/readings/Bidirectional%20Recurrent%20Neural%20Networks.pdf)

[Recurrent Neural Network Regularization](https://arxiv.org/pdf/1409.2329.pdf)

[LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf)

[analysis of eight LSTM variants](https://arxiv.org/pdf/1503.04069.pdf)

[GRU](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)

[ConvLSTM](https://papers.nips.cc/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf)

[Seq2Seq](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

[On the difficulty of training recurrent neural networks](http://proceedings.mlr.press/v28/pascanu13.pdf)

## 损失函数

[gradient descent ](https://arxiv.org/pdf/1609.04747.pdf)

## 优化器

### [leaky_relu ](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)

 ## GAN

[GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)



## 强化学习

[Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2020.pdf)



## Others

[A Maximum Entropy Approach to Natural Language Processing ](https://www.aclweb.org/anthology/J96-1002.pdf)

[Dropout](https://dl.acm.org/doi/pdf/10.5555/2627435.2670313)

[Batch Normalization](https://arxiv.org/pdf/1502.03167.pdf%20http://arxiv.org/abs/1502.03167.pdf)

# Survey

## 文本分类

[fastText](https://www.aclweb.org/anthology/E17-2068.pdf)

[A Survey of Deep Neural Network Architectures and Their Applications](https://bura.brunel.ac.uk/bitstream/2438/14221/1/FullText.pdf)

[Deep Learning in Neural Networks: An Overview](https://arxiv.org/pdf/1404.7828.pdf)

[On the Origin of Deep Learning](https://arxiv.org/pdf/1702.07800.pdf)

### [DeepIntent](https://www.kdd.org/kdd2016/papers/files/rfp0289-zhaiA.pdf)

[Word2Vec](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)

# Python

## 上下文管理器

```
1. 上下文表达式：with open('test.txt') as f:
2. 上下文管理器：open('test.txt')
3. f 不是上下文管理器，应该是资源对象
```

就是在一个类里，实现了`__enter__`和`__exit__`的方法，这个类的实例就是一个上下文管理器。使用了 with 语句，不管在处理文件过程中是否发生异常，都能保证 with 语句执行完毕后已经关闭了打开的文件句柄。

python os.path.basename()方法   返回path最后的文件名

 glob.glob 返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则，这里可以是绝对路径，也可以是相对路径。下面是使用glob.glob的例子：

```
import glob

#获取指定目录下的所有图片
print (glob.glob(r"/home/qiaoyunhao/*/*.png"),"\n")#加上r让字符串不转义

#获取上级目录的所有.py文件
print (glob.glob(r'../*.py')) #相对路径
```

#  数据处理

# 深度学习

## 基本概念

bach_size过小，往往使得在每一次迭代中，梯度的估计误差较大，bach_size过大，虽然梯度误差较小，但每一次的迭代更新时花费的算量过大，降低了优化效率。此外，考虑到计算机运算单元的特点，bach_size往往为2的整数幂，以较好地利用计算机的计算资源。

## 损失函数

考虑因素：

- 必须能够比较好地刻画和提现我们所期望的实际目标
- 它是否易于优化----股票预测时，均方误差是可微分函数，便于应用各种基于导数的优化算法，因此应用更广泛
- 交叉熵是一种刻画两个分布之间的差异的常用代价函数

## Dropout

相当于实现了一种多模型（被随机采样的子网络--当训练数据被输入网络时，网络会随机采样一个子网络进行前向传播及利用反向传播更新参数，因为某些神经元被丢弃）的集成。另一个角度，由于一个神经元不再固定地依赖某个特别的神经元的输入，也降低了神经元之间的强相关性。也就是说迫使网络学习那些更具鲁棒性的特征，即合神经元随机子集比较相关的特征，而不是和某个神经元。



## 框架



### [Tensorflow](https://wiki.jikexueyuan.com/project/tensorflow-zh/get_started/basic_usage.html)

- 设置参数并加载数据集
- 创建神经网络结构
- 定义训练函数，包括模型训练和模型存储
- 定义预测函数，包括模型导入和模型测试预测
- 创建main函数，让用户使用训练数据集进行训练，然后使用测试数据集进行预测

tf(1.12.0)

Keras有更高的准确率是因为其在Mnist数据集上对参数的初始化更合理

使用tensorflow实现机器学习模型训练的基本步骤

- 定义计算图。即定义一个从原始样本输入到最终输出的计算流程。
- 基本模型的输出及预测目标定义代价函数，并指定使用的优化算法优化此代价函数。
  - 监督学习，往往是将特征向量映射为某一个类别，并得到属于这个类别的置信度
  - 非监督学习，由于问题特点和目标往往有很大不同，
    - Auto-Encoder将特征向量映射回原始样本的输入空间，即进行样本重构，然后将重构样本与原始输入样本进行比较，计算其差异，并将重构误差作为代价函数
    - K-means聚类，将特征向量匹配到最近的一个聚类，并计算特征向量到此聚类中心的聚类。最终的代价函数是各样本到对应聚类中心的距离的平方和
- 运行优化算法学习模型参数。 前两步（定义计算流程图及代价函数）都是静态的，而第三步是开始计算，通过一轮轮的优化及迭代完成模型的训练。首先是读入数据，然后运行第一步中定义的计算图，并计算代价函数。之后运行优化算法更新模型参数，直到模型收敛完成模型参数的学习。

tf.placeholder()来定义输入，第一维的值为None，即表示矩阵的第一维长度可变；它是在程序运行时，根据实际的输入数据来决定的。由于神经网络是基于minibatch的，因此输入的第一维，一般是对应于minibatch中含有的样本个数（FALGS.batch_size）,因此常设置为None和FALGS.batch_size。





### Keras

### Pytorch

## RNN

应用：

- 语言模型、文本生成、语音识别、机器翻译、问答系统、股票预测

RNN中不同时刻的参数（W、U、V）都是共享的，降低自由参数个数，避免过训练，同时，它反映了RNN在不同时刻进行的操作（函数映射）是相同的，不同的仅是系统当时的状态及输入信号。

$S_{t}=\sigma({WS_{t-1}+UX{t})}$

$Y_t=\sigma(VS_t)$

BRNN

$S_{t}^1=\sigma({W^1S_{t-1}+U^1X{t})}$

$S_{t}^2=\sigma({W^2S_{t+1}+U^2X{t})}$

$Y_t=\sigma(V^1S_t^1+V^2S_t^2)$

误差信号会随着追溯的时间步长（对应神经网络的深度）呈指数减小，因此长距离依存关系（即很多步以前的信号变化对当前信号的影响）将丢失。这种**梯度消失**问题是训练RNN最根本的问题。通过引入新的激活函数以及新的网络结构可以在一定程度上缓解梯度消失问题。但都没从根本上解决这个问题

RNN可以将一个序列作为输入，通过其网络运算将输入序列映射为一个状态向量。好处是可以处理任意长的序列。缺点是无论输入序列有多长，包含多少信息量，最终都要被映射成唯一的一个状态变量。这意味着输入序列越长，包含的信息量越多，最终的状态向量就会丢失越多的信息。而含有注意力机制的RNN模型就是为了克服这一缺点。

事实上，RNN在依次处理输入序列的各个元素时，会对应产生一系列的中间状态向量，这些中间状态向量更好地保存了输入序列的信息。RNN模型直接把最后一个状态向量作为最终的表达向量，而含有注意力机制的RNN模型综合考虑了所有生成的中间状态变量。

### BRNN

BRNN保持了关于顺序的对称性，并且更加平衡地使用了前向和后向的上下文信息。无论是哪种具体的RNN模型，都可以将其隐藏状态向量$$h_t$$看成是第t个词的表达向量




### LSTM

引入LSTM是为了克服RNN不能有效建模长距离依存关系的问题

- 状态
  - 输入数据
  - 隐藏状态
  - 输入状态
  - 内部状态
    - 神经网络基于所有历史数据进行编码后所表达的信息总和
- 控制门--通过引入控制门，LSTM可以学会记住长距离的依存关系，也可以在必要时忘记过去。
  - 输入门
  - 遗忘门
  - 输出门

**这个地方留待补充**公式

LSTM的变体都不能显著改进标准LSTM，提示修改LSTM的单元内部结构不是一个好的方向，注意力模型是一个更有希望的方向

### GRU

简化LSTM，将内部状态和隐藏状态合并为同一个状态，并将遗忘门和输入门合并成为一个简单的更新门

### 含有注意力机制的RNN模型

在RNN的基础上，引入注意力机制，用以刻画各个词对于词序列整体语义的重要程度，进而学习得到整体次序列的最终表达向量。具体来讲，本算法的注意力机制，类似于池化。其作用都是将一系列的中间状态向量经过一定的运算，映射为一个最终的表达向量。不同之处在于，最大池化取序列中的最大值为结果，无法区分各个词对整体语义的重要程度。均值池化取序列中的均值为结构，lost pooling只以序列最后一个输出得到的状态向量作为最终的表达向量。注意力机制具有更好地动态自适应性，它根据序列中各元素的重要程度将其加权得到最终值。能更好地刻画长序列。

首先次序列经过词嵌入层被映射为嵌入向量序列，再经由RNN或BRNN映射为状态向量序列，最后通过注意力网络映射为最终表达向量

实现过程：输入层、词嵌入层、RNN实现、基于注意力机制的池化层实现（以RNN输出的中间状态结果为输入）、定义代价函数及优化算法、执行计算图进行计算。

计算图中定义的节点，有的属于数据节点，如loss，它对应一个tensor；有的是属于操作节点，如train_op，它实际上对应了一种操作。

## CNN

与普通的全连接前馈神经网络过来不同，cnn为局部连接，并且参数共享。所以自由参数比相同规模的全连接网络大大减小，很大程度上提高了模型的泛化能力。非常善于处理图像或语音等自然连续信号。

应用领域： 

- 信息检索、句法分析、文档摘要、实体检测

cnn的核心思想： 图像的像素在空间分布上有很强的相关性，图像可以由其局部特征的组合来表达；卷积运算是一种高效提取图像局部特征的手段，不同的卷积核（卷积核是卷积运算的模板）可以提前不同的局部特征；在应用同一个卷积核时，我们用这个卷积核遍历整个图像以抽取图像不同位置的局部特征

### AlexNet

- relu、GPU、存在重叠的池化感知域，最大池化，更好地提取**显著**特征，使得现状特征不会被平均池化弱化。

通过三种机制，卷积层可以很好地处理高维图像输入，并对 图像的位移和形变有较好的鲁棒性。

- 局部连接
- 参数共享
- 池化--对原始特征映射图进行了空间降采样
  - 降低后续操作的计算复杂度
  - 提高特征提取的鲁棒性

6个特征特征模板的卷积层（卷积核个数），相应它有6个特征映射图，对应6种特征提取方式。特征映射图（也即一种特征提取方式）上某一个位置的值是由输入图像对应位置为中心的感知域作为输入，经由此特征映射图对应的特征提取后得到的输出值。一旦特征被检测出来，其精确的位置信息不仅不太重要，甚至对识别的泛化性能可能是不利的。举例来说，一旦知道输入图像的左上部区域有一条大体水平的线段，右上部有拐角，右部有一条大致垂直的线段，那么将可以识别出为7.但是7在输入图像的位置可能有不同的差异



##　Seq2Seq

Seq2Seq除了在机器翻译方面的应用，还可以用来训练并学习任何类型的配对训练数据。

- 结合注意力

## GAN





## 词和文本模型

20世纪60年代，bag-of-words模型用于对文档进行建模。通常，可以计算两个文本向量的余弦相似度的值来度量 它们之间的相似程度。。70年代提出的TF-IDF算法，改进了词权重的计算方法。大幅度提高了文档相似度计算的准确率。

但它们几乎都是基于one-hot-vector来对词进行表示的，只是如何计算词的重要程度时有所不同，根本缺点是无法表达**词的语义相关度**。

Word2Vec模型：基于上下文的分布式表达

- Skip-Gram：对于每一个词，得到其上下文。与CBOW互为镜像

构建词典时，往往只会把训练语料库中出现次数大于一定阈值例如（5）的放入词典中。对于出现太少的，将其视为生僻词，用“UNK”表示。只挑选词频最高的vocabulary_size个词进行索引，构建词典。好处：

- 所有的词都放进词典，词典的索引值变得非常大
- 出现次数很少的词训练样本少，往往无法得到其准确的分布式嵌入向量

局限总结：

- 无法很好表达特点的短语。Best City 是一家电子设备零售商，其语义为一个公司名称。但是词汇Best 和city语义经过简单叠加或组合无法表达语义
- 基于上下文（当前词临近的一个小窗口内的词）信息学习词汇语义的特点，使它可能不足以表达一个词的完整语义，进而使学习的分布式嵌入向量存在一定的局限性。如：It was a bad weather,It was a good weather. 这样good和bad 具有类似的事上下文，这样词向量也会相近。
- 对每一个词只学习一个分布式向量表达，无法很好处理一词多义的问题
- 生僻词学到的分布式嵌入向量不准确。

### fastText



# [LaTeX ](https://www.mohu.org/info/symbols/symbols.htm)

# [LaTeX公式手册](https://www.cnblogs.com/1024th/p/11623258.html)

[用LaTeX 插入数学公式](https://blog.csdn.net/happyday_d/article/details/83715440)

[Latex基本表格绘制](https://blog.csdn.net/juechenyi/article/details/77116011)

注意表格内部换行，需要在文件定义部分加入...

```latex
\newcommand{\tabincell}[2]
  {
          \begin{tabular}
          {@{}#1@{}}#2
          \end{tabular}
  }
```

\Checkmark    %标准的勾  

\XSolid             %标准的叉  ----------------  \XSolidBrush   %加粗 \CheckmarkBold  \XSolidBold

\\\ 表示换行

１、指数和下标可以用^和_后加相应字符来实现。比如：

![img](https://www.mohu.org/info/symbols/foot.gif)

2、平方根（square root）的输入命令为：\sqrt，n 次方根相应地为: \sqrt[n]。方根符号的大小由LATEX自动加以调整。也可用\surd 仅给出
符号。比如：

![img](https://www.mohu.org/info/symbols/sqrt.GIF)

3、命令\overline 和\underline 在表达式的上、下方画出水平线。比如：

![img](https://www.mohu.org/info/symbols/overline.GIF)

4、命令\overbrace 和\underbrace 在表达式的上、下方给出一水平的大括号。

![img](https://www.mohu.org/info/symbols/brace.GIF)

5、向量（Vectors）通常用上方有小箭头（arrow symbols）的变量表示。这可由\vec 得到。另两个命令\overrightarrow 和\overleftarrow在定义从A 到B 的向量时非常有用。

![img](https://www.mohu.org/info/symbols/vec.GIF)

6、分数（fraction）使用\frac{...}{...} 排版。一般来说，1/2 这种形式更受欢迎，因为对于少量的分式，它看起来更好些。

![img](https://www.mohu.org/info/symbols/frac.GIF)

7、积分运算符（integral operator）用\int 来生成。求和运算符（sum operator）由\sum 生成。乘积运算符（product operator）由\prod 生成。上限和下限用^ 和_来生成，类似于上标和下标。

![img](https://www.mohu.org/info/symbols/int.GIF)

# 机器学习

# 杂谈



生成目录结构：  tree /f > list.txt



在搜索广告领域，对用户查询的搜索意图及广告商的广告意图进行精准匹配是至关重要的





## 全球十大突破性技术

- 2020
  - 防黑互联网、超个性化药物、数字货币、抗衰老药物、人工智能发现分子、超级星座卫星、量子优越性、微型人工智能、差分隐私、气候变化归因

- 2019
  - 灵巧机器人、核能新浪潮、早产预测、肠道显微胶囊、定制癌症疫苗、人造肉汉堡、捕获二氧化碳、可穿戴心电仪、无下水道卫生间、流利对话的AI助手

- 2018
  - 实用型3D金属打印机、人造胚胎、智慧传感城市、面向每一个人的人工智能、对抗性神经网络、巴别鱼实时翻译耳塞、零碳天然气、完美的在线隐私保护、基因占卜、材料的量子飞跃

- 2017
  - 强化学习、360°自拍、基因疗法2.0、太阳能热光伏电池、细胞图谱、自动驾驶货车、刷脸支付、 实用型量子计算机、治愈瘫痪、僵尸物联网

- 2016
  - 免疫工程、精确编辑植物基因、语音接口、可回收火箭、知识分享型机器人、DNA应用商店、Solar City的超级工厂、大规模集成各种工具和服务的团队协作工具、特斯拉自动驾驶仪、空中取电

- 2015
  - 3D虚拟现实、Apple Pay移动支付、汽车间通信、谷歌Project Loon、液体活体检查、脑细胞团培育、超动力的光合作用、DNA的互联网、大规模海水淡化、纳米架构

- 2014
  - 微型3D打印、基因组编辑、脑图谱、移动协同功公软件、头戴式显示器、超私密智能手机、智能并网发电、神经形态芯片、农业无人机、敏捷机器人
