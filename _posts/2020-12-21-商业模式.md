---
layout: post
title: 商业模式
mathjax: true
tags: business
author: Wxl
date: 2020-12-21
header-style: text
excerpt_separator: <!--more-->
---

<!--more-->

[基于视频的招聘平台](https://techcrunch.com/2020/12/09/myinterview-raises-5-million-for-its-video-based-job-recruitment-platform/)

[数据集](https://techcrunch.com/2020/12/18/v7-labs/)

[AI模型监控](https://techcrunch.com/2020/12/09/arthur-ai-snags-15m-series-a-to-grow-machine-learning-monitoring-tool/)

[机器学习功能商店](https://techcrunch.com/2020/12/07/tecton-ai-nabs-35m-series-b-as-it-releases-machine-learning-feature-store/)

[农业自动化](https://techcrunch.com/2020/12/14/tencent-invests-in-dji-backed-agritech-startup-fj-dynamics/)

[草坪护理](https://techcrunch.com/2020/12/08/lawn-startup-sunday-raises-millions-to-help-you-with-your-backyard/)

[理发](https://www.ibisworld.com/industry-statistics/market-size/barber-shops-united-states/):https://techcrunch.com/2020/12/09/squire-series-c/

[欠条说](http://www.51qiantiao.com/#)

大家好，今天我给大家分享的两个案例是Arthur AI 和myinterview。首先给大家介绍一下Arthur AI。

这是一个用于监视机器学习模型的生产性能的平台。在越来越多的公司正在构建机器学习模型的时候，

机器学习是一个复杂的过程。第一步是建立一个模型，在实验室条件下对其进行训练、测试，然后将其部署到其他地方。在这之后，如何跟踪机器学习模型的性能特别重要。而且随着人工智能的发展，高科技公司的算法越来越受到数据驱动，模型上线后的结果是否还会按照预期表现良好，也是公司非常关注的问题。因此，Adam Wenchel等人在2018年创建了Arthur AI这家公司，总部位于纽约州纽约市。

现在Arthur AI帮助金融服务，保险和医疗保健等行业的公司开发和部署企业级AI系统。

Arthur AI在19年8月份宣布获得了由Work-Bench领投的330万美元的种子轮融资，在这个月宣布获得了由Index Ventures领投的1500万美元的A轮融资。

可以看到，全球知名的市场研究机构 CB Insights 发布的CB Insights AI 100榜单上，Arthur AI在这个进入了前100名，属于模型检测行业。

它的联合创始人Adam Wenchel拥有马里兰大学的计算机科学学士学位。他先是创立了Anax Security，这家公司是一家致力于大规模防御性网络安全的机器学习的初创公司。在Anax被Capital One收购之后，Adam担任Capital One的AI和数据创新副总裁，从事AI的公平性和可解释性等研究。然后在2018年创立了Arthur，这家初创公司的主要人员也是来自Capital One。

模型监测。ArthurAI提供了对模型性能和结果的实时可见性，并在模型表现不理想时发出警告。Arthur AI希望确保模型的准确性不会随着时间的流逝而开始下滑，从而失去精确测试其应有能力的能力。

它可以集中监控所有模型，无论是部署在云上，在数据中心中还是在多个平台之间。因此，Arthur AI使集中监控变得非常简单。

下面介绍它的主要功能，第一个是性能优化。可以使用任何指标来跟踪模型的性能，以发现是否有改善模型性能的机会，并在数据模型的预测之前检测到是否发生了数据漂移问题。

第二个是偏差检测，找出模型中不想要的偏差，并获得有关纠正偏差的建议，以确保模型的正确性，并公平地对待所有客户。

第三个是可解释性。就是通过观察模型预测的结果是否和预期的一致。

最后介绍它的两个应用。

在金融服务行业，金融科技公司TrueBill依靠ArthurAI监控其LTV和客户流失模型的性能，在出现实际问题之前捕获并修复数据漂移。

在医疗保健行业，健康保险提供商Humana正在使用ArthurAI来监视和减轻其预防保健模型中的偏见，以确保公平和可解释的结果。

下面介绍的是myinterview

制作简历比较麻烦，招聘人员也很难在简历中了解到你的个性，公司招聘人才的方式正在发生根本性的转变。因此，在2016年的时候Guy Abelsohn和Ben Gillman在以色列创立了myinterview。myInterview最初是为公司提供产品以集成到其现有的招聘系统中，在2019年初推出了一个独立的平台。myInterview是一个基于视频的招聘平台。求职者可以上传对问题的视频回复，表达自己的真实个性，经验和能力。招聘人员还可以选择使用myInterview Intelligence™或基于机器学习的工具，可以为工作岗位提供相应的求职者的候选名单列表。减少招聘时间，使招聘流程更加高效。

可以看到myInterview在2018年已经筹集了160万美元的种子前融资。在这个月获得了500万美元的种子融资。到目前为止，myInterview已被2,000多家公司和200万求职者使用。其中大部分在美国和英国。

myinterview提供了优质视频面试技术
myInterview是专为招聘人员而设计
myInterview非常灵活。 可以将myInterview集成到ATS / HR系统中或用作独立产品。

它的一些客户包括Facebook和麦当劳

myInterview有哪些优势呢？第一个是

评估真实性：您可以信任的应用程序。 视频是真实的。
公司文化：寻找有助于您公司目标和价值观的候选人。
候选洞察：让数据帮助您做出决定。 客观地筛选和筛选候选对象。
合作：与您的同事轻松协作，找到最匹配的人。

lmyInterview与竞争对手竞争的主要方式之一是myInterview Intelligence。如果招聘人员使用myInterview Intelligence，则该平台会分析求职者回复的关键字，短语和语气，就是做一些自然语言处理的工作。

下面介绍一下它的收费情况。

一共有5个版本，入门版是永久免费，但是功能有限。然后是按年付费，这个给的是每个月的价格。最后一个是私人订制。

表现包括： 24个在招职位 15个用户座位 8400名候选人/年 团队中的一切加上： 12实时访谈/月 视频转录 myInterview Intelligence™ 小部件集成 高级招聘人员分享 进阶分析 入职培训 SLA等级支持

[Explainable Machine Learning for Scientific Insights and Discoveries](https://arxiv.org/pdf/1905.08883.pdf)

[A Tutorial on Fairness in Machine Learning](https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb)

内容基于：Solon Bacrocas和Moritz Hardt在NIPS2017[上发表的关于公平的教程，](https://nips.cc/Conferences/2017/Schedule?showEvent=8734)由UC Berkeley的Moritz Hardt教授的[CS 294：《机器学习中的公平》的第](https://fairmlclass.github.io/)1天和第4天，以及我自己对公平文献的理解。我强烈鼓励感兴趣的读者查看链接的NIPS教程和课程网站。

人们普遍误以为人工智能是绝对客观的。人工智能仅在学习人类教义的意义上是客观的。人类提供的数据可能会有很大偏差。2016年发现，用于惯犯预测的算法COMPAS对黑人产生的误报率要比白人高得多

Xing是一个类似于Linked-in的工作平台，被发现对合格的男性候选人的排名高于对合格的女性候选人的排名

分别由Microsoft，Face ++和IBM提供的可公开获得的商业人脸识别在线服务在肤色较深的女性身上的准确率要低得多

当应用程序涉及人时，ML中的偏差几乎无处不在，并且已经损害了少数群体或历史上处于不利地位的群体的人们的利益。不仅少数族裔的人，每个人都应该关注AI的偏见。

是什么导致机器学习系统出现偏见？” 本质上，由于历史原因，偏差来自训练数据集中存在的人为偏差。以下是潜在原因的列表

**偏斜样本**：如果偶然出现一些初始偏差，则这种偏差可能随时间推移而加剧：未来的观察结果可以证实预测结果，而做出与预测相矛盾的观察结果的机会则更少。一个例子是警察记录。犯罪记录仅来自警察观察到的那些犯罪。警察部门倾向于派遣更多的警员到最初发现犯罪率较高的地方，因此更有可能在这些地区记录犯罪。即使其他地区的人后来的犯罪率较高，也可能由于警方的关注较少，警察部门仍然记录这些地区的犯罪率较低。使用以这种方式收集的数据训练的预测系统倾向于对警察较少的区域产生积极的偏见。

**受污的例子**：任何机器学习系统都会保留由于人为偏见而导致的旧数据中存在的偏见。例如，如果系统使用经理做出的聘用决定作为标签来选择应聘者，而不是求职者的能力（大多数情况下，这种能力对于被拒绝的人是不可见的）。使用这些样本训练的系统将复制经理决策中存在的偏差（如果有）。另一个例子是，在Google新闻文章上训练的词嵌入“以令人不安的程度展现了男女的刻板印象”，例如，发现“男人”与“计算机程序员”之间的关系与“女人”与“家庭主妇”之间的关系高度相似（[Bolukbasi等人，2016年](https://arxiv.org/pdf/1607.06520.pdf)）。

**功能有限**：对于少数群体而言，功能的信息量可能较少，或者可靠地收集了。如果来自少数群体的标签的可靠性远低于来自多数群体的标签的可靠性，则由于这些噪声，系统在预测少数群体时的准确性往往会低得多。
**样本规模差异**：如果来自少数群体的培训数据远少于来自多数群体的培训数据，则不太可能完美地建模少数群体。
**代理**：即使不使用敏感属性（考虑的属性不应该用于任务，例如种族/性别）来训练ML系统，也始终可以存在其他一些属性作为敏感属性的代理（例如邻域）。如果包含这些功能，则仍然会发生偏差。有时，很难确定某个相关功能是否与受保护的功能过于相关，以及是否应将其包含在培训中。

它们可以分为以下三个问题：

- 发现性能上无法观察到的差异：示例偏斜，示例受污染
- 具有观察到的性能差异的样品处理：功能有限，样品量差异
- 了解预测结果差异的原因：代理

# 公平的定义

数据的公平性和机器学习算法对于从头开始构建安全负责的AI系统至关重要。技术和商业AI利益相关者都在不断追求公平，以确保他们有意义地解决AI偏差等问题。尽管准确性是评估机器学习模型准确性的一个指标，但公平性为我们提供了一种了解在现实环境中部署模型的实际含义的方法。 

公平是理解数据引入的偏差并确保模型在所有人口统计群体中提供公平预测的过程。与其将公平视为一个单独的计划，不如在整个机器学习过程中应用公平分析，以确保从公平和包容的角度不断重新评估模型，这一点很重要。当将AI部署在影响广泛终端用户的关键业务流程（例如信贷申请审查和医疗诊断）中时，这一点尤其重要。 

例如，以下是典型的ML生命周期：

![1个典型的ML lifecycle.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/1_typical_ML_lifecycle.max-900x900.png)![img](https://cloud.google.com/blog/)

下面以黄色显示了可以在模型开发的各个阶段应用ML公平性的一些方法：

![2模型开发.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/2_model_development.max-1900x1900.png)![img](https://cloud.google.com/blog/)

与其将已部署的模型视为流程的结尾，不如将上面概述的步骤视为一个循环，在该循环中，您将不断评估模型的公平性，添加新的训练数据并进行再训练。部署完模型的第一个版本后，最好收集有关模型性能的反馈，并在下一次迭代中采取措施提高其公平性。

在此博客中，我们将重点关注上图中的前三个公平步骤：识别数据集不平衡，确保公平对待所有组以及设置预测阈值。为了给您提供从公平性角度分析模型的具体方法，我们将对在AI平台上部署的模型使用[假设分析工具](https://pair-code.github.io/what-if-tool/index.html#demos)。我们将特别关注假设分析工具的“**公平性和性能”**选项卡，该选项卡使您可以按单个功能对数据进行切片，以查看模型在不同子集上的行为。