---
layout: post
title: Loss Fuction
subtitle: 损失函数
tags: loss 机器学习
mathjax: true
date: 2020-11-23
header-style: text
author: wxl
excerpt_separator: <!--more-->
---

[Losses](https://keras.io/api/losses/)

# 合页损失函数 Hinge Loss for "maximum-margin" classification

```
tf.keras.losses.Hinge(reduction="auto", name="hinge")
```

Computes the hinge loss between `y_true` and `y_pred`.

```
loss = maximum(1 - y_true * y_pred, 0)
```

**`y_true` values are expected to be -1 or 1.** If binary (0 or 1) labels are provided we will convert them to -1 or 1.

Standalone usage:

```python
>>> y_true = [[0., 1.], [0., 0.]]
>>> y_pred = [[0.6, 0.4], [0.4, 0.6]] 
>>> # Using 'auto'/'sum_over_batch_size' reduction type.  
>>> h = tf.keras.losses.Hinge() 
>>> h(y_true, y_pred).numpy() 1.3
```

1）0-1损失

当样本被正确分类时，损失为0；当样本被错误分类时，损失为1。

2）感知机损失函数

当样本被正确分类时，损失为0；当样本被错误分类时，损失为-y(wx+b)。

3）合页损失函数

当样本被正确分类且函数间隔大于1时，合页损失才是0，否则损失是1-y(wx+b)。

相比之下，合页损失函数不仅要正确分类，而且确信度足够高时损失才是0。也就是说，合页损失函数对学习有更高的要求。

hinge loss相对于**感知机**的loss向右平移了一个单位，相当于不再单单要求分类正确，还对每个点离分类平面的距离有一定的要求。

![.mw-parser-output .serif{font-family:Times,serif}](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Hinge_loss_vs_zero_one_loss.svg/440px-Hinge_loss_vs_zero_one_loss.svg.png)

t = 1 时变量 y（水平方向）的铰链损失（蓝色，垂直方向）与0/1损失（垂直方向；绿色为 y < 0 ，即分类错误）。注意铰接损失在 abs(y) < 1 时也会给出惩罚，对应于支持向量机中间隔的概念。

在[機器學習](https://www.wikiwand.com/zh/机器学习)中，**鉸鏈損失**是一個用於訓練分類器的[損失函數](https://www.wikiwand.com/zh/损失函数)。鉸鏈損失被用於「最大間格分類」，因此非常適合用於[支持向量機](https://www.wikiwand.com/zh/支持向量机) (SVM)。[[1\]](https://www.wikiwand.com/zh/Hinge_loss#citenote1) 对于一个预期输出 ![{\displaystyle t={\pm }1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/33a77f2728fba9269cf5fc4d73ad65a07633dbbe)，分类结果 ![y](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d) 的鉸鏈損失定義為



特別注意：以上式子的![y](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d)應該使用分類器的「原始輸出」，而非預測標籤。例如，在線性支持向量機當中，![{\displaystyle y=\mathbf {w} \cdot \mathbf {x} +b}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c430c17f27e7d7dd52e5a12741eceacc355d87d5)，其中 ![{\displaystyle (\mathbf {w} ,b)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a11219e8065d442b9e95cc1f161ad4ff9d2c273e) 是[超平面](https://www.wikiwand.com/zh/超平面)参数，![\mathbf {x} ](https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2)是輸入資料點。

當![t](https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560)和![y](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d)同號（意即分類器的輸出![y](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d)是正確的分類），且 ![{\displaystyle |y|\geq 1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e949835f150dfe886776296b912457c704c17697)时，鉸鏈損失 ![{\displaystyle \ell (y)=0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cd23c92c905a4fd0e2ec0beb6c21e32e4f119e2f)。但是，當它們異號（意即分類器的輸出![y](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d)是正確的分類）時，![{\displaystyle \ell (y)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a247d45484db98651f830fd511a6927ed6caa5bc) 隨 ![y](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d) 線性增長。套用相似的想法，如果 ![{\displaystyle |y|<1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5a7a1cfbe0bc926e313771f06b0dda3cdbee363c)，即使 ![t](https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560) 和 ![y](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d) 同號（意即分類器的分類正確，但是間隔不足），此時仍然會有損失。