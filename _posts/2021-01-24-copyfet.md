---
layout: post
title:  FET
mathjax: true
tags: fet 毕业论文
author: Wxl
date: 2021-01-24
header-style: text
---

左边换成word







BaselinesTo demonstrate the effectiveness of our model, we compareresults with the following baselines

Transe(Bordes et al. 2013): one of the most widely usedKGC models

Distmult (Yang et al. 2015): a popular tensor factorization based model which uses a bilinear score function tocompute scores of knowledge triples.

Complex(Trouillon et al 2016): an extension of DistMult which embeds entities and relations into complexvectors instead of real-valued ones

Rotate(Sun et al. 2019): a state-of-the-art model whichdefines each relation as a rotation from the head entity tothe tail entity in the complex vector space

Conve (Dettmers et al. 2018): a popular convolutionalnetwork based model

CONVKB (Nguyen et al. 2018): another state-of-the-artconvolutional network based mode

R-GCN (Schlichtkrull et al. 2018): an extension ofgraph convolutional network, which can effectively modelmulti-relational data

A2N (Bansal et al. 2019): a recent model that learnsquery-dependent representations of entities based on aGNN structureNathani's(Nathani et al. 2019): a recent model that moIs the local neighborhood via graph attention network.







Our model improves the performances by a large margin compared to the basic model () on all three datasets.
Specifically, the auto-relabeling enhanced model improves the strict accuracy on the three datasets from 68.9 to 70.1  



**FIGER**: Ling and Weld (2012) sampled sentences from 780k Wikipedia articles and 434 news reports to form the train and test data respectively, and annotated entities using 113 types. The splits we use are the same in Shimaoka et al. (2017).  

**FIGER**(GOLD): The training data consists of Wikipedia sentences and was automatically generated with distant supervision, by
mapping Wikipedia identifiers to Freebase ones. The test data, mainly consisting of sentences from news reports, was manually
annotated as described by Ling and Weld (2012).  

**Wiki**/FIGER(GOLD): The training data consists of Wikipedia sentences and was automatically generated in distant supervision paradigm, by mapping hyperlinks in Wikipedia articles to Freebase. The test data, mainly consisting of sentences from news reports, was manually annotated as described in (Ling and Weld, 2012).  

**FIGER** Ling and Weld (2012) sampled a dataset from Wikipdia articles and news reports to form the train and test data respectively, and annotated entities using 113 types with a 2-level hierarchy . 

**FIGER**(Ling and Weld, 2012), derived from Wikipedia, uses 113 types with a 2-level hierarchy (e.g.,/person/musician). We use the same splits(2M train / 10k dev / 563 test) as (Shimaoka et al., 2017).

 BBN (Weischedel and Brunstein, 2005) is based on a portion of the one million word Penn Treebank corpus from Wall Street Journal articles and is completely manually annotated using using a 2-level hierarchy.We use the same splits (84k train / 2k dev / 14k test) as Ren
et al. (2016b); Chen et al. (2020).  

**Wiki/Figer:** Its training data consists of Wikipedia sentences automatically labeled via distant supervision by mapping the entity mentions to Freebase types (Bollacker et al. 2008). The testing data consists of news reports manually labeled by (Ling and Weld 2012)  

uses bi-directional LSTM to encode the context information, followed by attention to mechanism to select informative information. is an attentive neural network based model;

We conduct experiments on two commonly used dataset. The experimental results demonstrates the effectiveness of our approach.  

Experimental results on two commonly used dataset demonstrate that the proposed model  the effectiveness of our approach and outperforms previous state-of-the-art methods significantly  



used an averaging encoder to encode the entity mention, bi-directional LSTM to encode the
context, followed by attention to attend over label-specific
context.   

## 4.1  Datasets

**BBN**: It consists of sentences from the Wall Street Journal annotated by (Weischedel and Brunstein 2005). The training data is annotated using DBpedia Spotlight.  

**BBN** Weischedel and Brunstein (2005) labeled a portion of the one million word Penn Treebank
corpus of Wall Street Journal texts (LDC95T7) using a two-level hierarchy, resulting in the BBN Pronoun Coreference and Entity Type Corpus. We follow the train/test split by Ren et al. (2016b), and follow the train/dev split by Zhang et al. (2018)    

**BBN**: BBN dataset consists of sentences from Wall Street Journal articles and is completely manually annotated using 93 types.(Weischedel and Brunstein, 2005). Please refer to (Ren et al., 2016) for more details  of the datasets  

**BBN**: Weischedel and Brunstein (2005) annotated entities in Wall Street Journal using 93 types. We use the train/test splits in Ren et al. (2016b) and randomly hold out 2,000 pairs for dev. Document contexts are retrieved from the original corpus.

**Wiki/Figer:** Its training data consists of Wikipedia sentences automatically labeled via distant supervision by mapping the entity mentions to Freebase types (Bollacker et al. 2008). The testing data consists of news reports manually labeled by (Ling and Weld 2012)  

**FIGER** Ling and Weld (2012) sampled a dataset from Wikipdia articles and news reports. Entity mentions in these texts are mapped to a 113-type ontology derived from Freebase (Bollacker et al., 2008). Again, we follow the data split by Shimaoka et al. (2017)  

**FIGER**(Ling and Weld, 2012), derived from Wikipedia, uses 113 types with a 2-level hierarchy (e.g.,/person/musician). We use the same splits(2M train / 10k dev / 563 test) as (Shimaoka et al., 2017). BBN (Weischedel and Brunstein, 2005) is based on the one million word Penn Treebank corpus from Wall Street Journal articles. We use the same splits (84k train / 2k dev / 14k test) as Ren
et al. (2016b); Chen et al. (2020).  

**FIGER**: Ling and Weld (2012) sampled sentences from 780k Wikipedia articles and 434 news reports to form the train and test data respectively, and annotated entities using 113 types. The splits we use are the same in Shimaoka et al. (2017).  

**FIGER**(GOLD): The training data consists of Wikipedia sentences and was automatically generated with distant supervision, by
mapping Wikipedia identifiers to Freebase ones. The test data, mainly consisting of sentences from news reports, was manually
annotated as described by Ling and Weld (2012).  

**Wiki**/FIGER(GOLD): The training data consists of Wikipedia sentences and was automatically generated in distant supervision paradigm, by mapping hyperlinks in Wikipedia articles to Freebase. The test data, mainly consisting of sentences from news reports, was manually annotated as described in (Ling and Weld, 2012).  

## 4.2  Baselines

To demonstrate the effectiveness of our proposed models, we compare \textsc{CopyFet} with several state-of-the-art FET systems:the followingbaselines.•AFET (Ren et al., 2016a):•We compare NDP with several state-of-the-art neural network models:  

We compare the proposed method with several state-of-the-art FET systems3:  

We compare our method with several state-ofthe-art FET systems, including AFET [Ren et al., 2016a], AAA [Abhishek et al., 2017], Attentive [Shimaoka et al., 2016], NDP [Wu et al., 2019], NFETC [Xu and Barbosa, 2018] and NFETC-CLSC [Chen et al., 2019]. [Xu and Barbosa, 2018] propose to model the type hierarchy with the hierarchical loss, which has been proven effective for FET task.
Thus, for all NFETC based methods, we report results produced by NFETC and NFETChier respectively.  

We compare NDP with several state-of-the-art neural network models:   

We compare FGET-RR with the existing state-of-the-art research on FG-NET, namely: (i) FIGER (Ling and Weld
2012), (ii) HYENA (Yosef et al. 2013), (iii) AFET (Renet al. 2016a) and its variants AFET-NoCo, AFET-NoPa,AFET-CoH, (iv) Attentive (Shimaoka et al. 2016), (v)FNET (Abhishek, Anand, and Awekar 2017), and (vi)NFGEC+LME (Xin et al. 2018) 1. For all these models,
we use the scores reported in the published papers, as theyare computed using the same data settings as that of ours  



model the samples with only one label and samples with multiple labels separately to improve the prediction performance   

utilizes type information from KB obtained through entity linking to form   the final feature vector of mention e refines the  the feature vector of mentionnoisy mention representations prior to the end classification.

 utilizes type information from KB obtained through entity linking  the feature vector of mention m is
    formed by concatenating its mention representation m and context representation c\





## 4.3  Evaluation Protoco

On \textsc{CopyFet}, we evaluate the performance by Strict Accuracy (Strict Acc), loose macro F-score(MacroF1)  and loose micro F-score (Micro-F1) , which is the most widely used evaluation setting for FET systems

Strict accuracy, loose macro and loose micro (Lingand Weld, 2012) Macro-averaged  P/R/F1  

We adopt the same criteria as Ling and Weld (2012), that is, we evaluate the model performance by strict accuracy, loose macro, and loose micro scores.  

We adopt three commonly used evaluation metrics: Strict Accuracy (Strict Acc), Micro-averaged F1 (Micro-F1) score and Macro-averaged F1 (MacroF1) [Ling and Weld, 2012]. On all datasets, we use the same training/development/test sets settings with previous studies, and the development sets are used to select the model with the best performance among all epochs  



For evaluation metrics, we evaluate the performance by strict accuracy, loose macro F-score and loose micro F-score,
which is the most widely used evaluation setting for FET systems [Ling and Weld, 2012].  

**Table 3 shows the hyperparameters used in our method. The model is trained using mini-batched back-propagation, and Adam optimizer [Kingma and Ba, 2014] is used for optimization.**  

Following (Shimaoka et al. 2017), we use pre-trained word embeddings from (Pennington, Socher, and Manning 2014). We use Adam Optimizer (Kingma and Ba 2014) and mini-batch of size B for parameter optimization. We also use implementation of TransE from (Lin et al. 2015) to obtain entity embeddings.
To avoid overfitting, we employ Dropout (Srivastava et al. 2014) on entity mention representation. The reason for only applying Dropout to entity mention is that entities of test set are probably unseen in training, while context words are not so different.
We explore different sets of hyperparameter settings and determine Adam optimizer learning rate λ among {0.01,
0.005, 0.001}, hidden-size of LSTM among {100, 150,200}, word vector size among {50, 100, 300}, window size
L among {5, 10, 15} and batch size B among {100, 500,1,000}, based on performance on the validation set. The hyperparameter settings are shown in Table 3.  



We use the pre-trained original-5.5b ELMo model 2 and freeze its weights during training. We use an Adam optimizer with learning rate of 5e- 5, L2 weight decay of 0.01, warmup rate of 0.1, and linear learning rate decay. We use a minibatch size of 200. To reduce overfitting, we apply dropout (Srivastava et al., 2014) to the word representation, relative position term, and the final
feature vector with probability 0.5, 0.2, and 0.2.  

We use open-source GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B with 300 dimensions to initialize word embeddings used in all encoders. All weight parameters are sampled from U(-0.01, 0.01). The encoder for sentence-level context is a 2-layer bidirectional RNN with 200 hidden units. The DM output size is 50. Sizes of Wa, Wd1 and Wd2 are
200×300, 70×50, and 50×70 respectively. Adam optimizer (Kingma and Ba, 2014) and mini-batch gradient is used for optimization. Batch size is \200. Dropout (rate=0.5) is applied to three feature functions. To avoid overfitting, we choose   

## 4.4  Results and Analysis

We speculate that ..., speculative reason is that be-cause

The comparison results are presented in Table 3, where best performance is highlighted in bold. The performances of
the compared models are all taken from the literature.  

Table 1 presents the final results of our method with three metrics. The scores of each metric are calculated by running
the model for five times and computing the mean and standard deviation values. We highlight the best performances of each
metric in bold. We find that our approach achieves the best performance among all the baselines.
Our model improves the performances by a large margin compared to the basic model (NFETC) on all three datasets.
Specifically, the auto-relabeling enhanced model improves the strict accuracy on the three datasets from 68.9 to 70.1,  



We compare the performance of our model with state-of-the-art methods on OntoNotes, FIGER, and KNET in Table 2, 3, and 4.  

# AFET2016

4.1 Data Preparation
Datasets. Our experiments use three public datasets.
(1) Wiki (Ling and Weld, 2012): consists of 1.5M sentences sampled from Wikipedia articles;

 (2)OntoNotes (Weischedel et al., 2011): consists of 13,109 news documents where 77 test documents
are manually annotated (Gillick et al., 2014); 

(3)BBN (Weischedel and Brunstein, 2005): consists of 2,311 Wall Street Journal articles which are manually annotated using 93 types. Statistics of the datasets are shown in Table 3.
**Training Data**. We followed the process in (Ling and Weld, 2012) to generate training data for the Wiki dataset. For the BBN and OntoNotes datasets, we used DBpedia Spotlight3 for entity linking. We discarded types which cannot be mapped to Freebase types in the BBN dataset (47 of 93). Table 2 lists the set of features used in our experiments, which are similar to those used in (Yogatama et al., 2015; Ling and Weld, 2012) except for topics and ReVerb patterns. We discarded the features which occur only once in the corpus.  

# Attentive2016



# NFGEC2017

在上下文表示的编码上采用了循环神经网络加自注意力机制的方法，为后期的研究打下了重要基础 。并利用远程监督构建了 OntoNotes 数据集。之后，许多工作开始研究噪声数据的应用问题 。

Shimaoka 等人[13,14]在上下文表示的建模上采用了长短时记忆网络，然后利用自注意力机制（self attention mechanism） 来融合得到整个文本片段的特征表示，使得模型更加关注与分类相关的词汇片段。在提及表示上则采用了提及所有词向量的平均。 

**For evaluation**, in our experiments we use the two well-established **manually annotated**
**datasets** FIGER (GOLD) and OntoNotes, where like Gillick et al. (2014), we discarded pronominal mentions, resulting in a total of 8, 963 mentions. 

**For training,** we use the automatically ind                                                                        uced publicly available datasets provided by Ren et al. (2016). Ren et al. (2016) aimed to eliminate label noise generated in the process of distant supervision and we use the “raw” noisy data2 provided by them for training our models  

# AAA2017



# KNET2018



# NFETC2018



# NFGEC-LME2018



# UFET2018

# NFETC-CLSC2019



# DenoiseET2019



# HFET2019(attentive)

# NFETC-VAT2020

5.1 Dataset
We evaluate out method on two benchmarks: Ontonotes andBBN. Statistics of the datasets are shown in Table 1.
**Ontonotes**. The Ontonotes dataset is derived from the Newswire part of Ontonotes corpus and annotated by [Gillicket al., 2014]. The training set of Ontonotes is annotated utilizing DBpedia spotlight, while the test set is manually labeled.
**BBN**. The BBN dataset is derived from 2,311 Wall Street Journal articles. We use the version processed by [Ren et al.,
2016a].
In this work, we use the preprocessed dataset provided by [Chen et al., 2019; Onoe and Durrett, 2019].  

# FGET-RR2020

# NFETC-AR2020



# ACL2020

# NDP2019

# EFGET2019

# IFETET2019