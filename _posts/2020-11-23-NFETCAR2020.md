---
layout: post
title: Learning with Noise
subtitle: NFETC2018
mathjax:true
tags: FGET 论文翻译
author: Wxl
date: 2020-11-23
header-style: text
excerpt_separator: <!--more-->
---



改进缺点：**仍然依赖一些先决条件**（人为构造干净数据集）；**无法消除仅带有一种类型标签但假阳性的样品的影响**

方法：对标签进行重标注---在训练过程中有效地重新标记嘈杂的远程样本。

<!--more-->

# Learning with Noise: Improving Distantly-Supervised Fine-grained Entity Typing via Automatic Relabeling  
# Abstract  

细粒度实体类型（FET）是各种利用实体的应用程序的基本任务。尽管已经取得了巨大的成功，但是现有系统在处理由远程监管方法引入的训练数据中的噪声样本方面仍然存在挑战。为了解决这些噪声，先前的研究要么集中于**以不同的策略处理**clean samples（即仅具有一个标签），和noisy samples（即具有多个标签），要么基于远程监督的标记集肯定包含正确类型的标记的假设来**过滤有噪声的标记**。在本文中，我们提出一种概率自动重新标记方法，可以**对所有训练样本进行统一处理**。我们的方法旨在估计每个样本的(pseudo-truth label distribution  )伪真标签分布，并且(pseudo-truth distribution  )伪真分布将被视为可训练参数的一部分，这些参数会在训练过程中一起更新。所提出的方法**不依赖任何先决条件或额外的监督**，从而使其在实际应用中有效。在多个基准上进行的实验表明，我们的方法优于以前的有竞争力的方法，确实减轻了嘈杂的标签问题。

# 1 Introduction  

细粒度实体类型（FET）是的任务旨在根据给定的实体提及及其相应的上下文文本来找到适当的细粒度语义类型。通过FET获得的知识具有参考价值，可以使多种自然语言处理（NLP）应用受益，例如关系提取、知识扩展、事实性问题解答，以及实体链接（Onoe and Durrett，2019a） [^1 ]。
由于缺少手动注释的细粒度标签，因此，最近的FET系统普遍采用了远程监督[Mintz et al., 2009] [^2]方法。此方法将实体提及链接到知识库中的实体，并将所有关联的类型注释为远程监督的标签。尽管远程监督足以自动标记数据，但它严重遭受了嘈杂的标记问题。如图1所示，两个不同句子中的实体提及“亚马逊”将用相同的实体类型集标记，其中某些类型在给定上下文下是不合适的。显然，直接远距离监督会产生嘈杂的训练数据，这将损害FET系统的性能[Ren et al., 2016a] [^3]。

![image-20201123135602803](/assets/fet/image-20201123135602803.png)

为了解决嘈杂标签的问题，以前的大多数研究都试图分别对clean samples和noisy samples进行建模，以提高预测性能[Ren et al., 2016a; Abhishek et al., 2017;Xu and Barbosa, 2018; Chen et al., 2019  ] 。实际上，我们发现只有**一个远距离标签**（clean sampel）的样本并不是绝对的 正确。 [Wu et al.，2019]提出了一种假设：即对每个训练样本，在远程监督的标签集一定包含正确类型的前提下，对噪声进行检测和加权，该假设过强。 [Onoe and Durrett，2019b]建议使用在人类标注数据上训练的学习模型来完善远程标注的数据。因此，以前的研究有两个局限性：1）**仍然依赖一些先决条件**（例如，人为整理的干净数据集），使其在实际应用中效率低下； 2）**无法消除仅带有一种类型标签但假阳性的样本的影响**（表4显示更多详细信息）。

为了同时解决上述两个局限性，本文提出了一种**概率自动重标记方法**。**由于真实标签分布不可用，因此我们的方法旨在在训练过程中估计伪真标签分布**。具体来说，为每个样本分配所有候选标签上的连续标签分布$\tilde p$_，并且将通过反向传播算法将$\tilde p$_作为可训练参数共同更新。学习目的是最小化**预测分布**和**伪真标签分布**之间的Kullback-Leibler（KL）差异。**最后，我们将$\tilde p$中具有最高值的标签作为唯一的伪真标签。**为了确保最终估计的伪真分布的合理性，我们在训练过程中将(golden-noisy information)黄金噪声信息与两个特定的设计约束进行了整合。这样，我们的方法可以**在训练过程中有效地重新标记嘈杂的远程样本**，从而提高预测性能。

为了展示我们方法的有效性和鲁棒性，我们在三个基准上进行了实验。 实验结果表明，我们的方法取得了最先进的结果，大大优于以前的方法。 此外，我们设计了其他辅助实验，以证明在使用噪声数据训练的FET任务上可以缓解上述问题。

# 2 Our Approach  

我们提出的模型的总体架构如图2所示。具体来说，我们的模型包括一个特征编码器（**其结构和目标函数与[Xu and Barbosa，2018]提出的NFETC模型相同**）和一个概率自动重新标记模块 ，以及一个三阶段的训练策略。

![image-20201123135355124](/assets/fet/image-20201123135355124.png)

## 2.1 Problem Definition  

给定一个训练语料库$D$，该训练语料库$D$由远程监督标记为知识库的实体类型层次结构，我们定义$\Gamma=t_1,t_2,..,t_{ \lvert T\rvert }$作为所有候选类型标签，其中$t_{\lvert T\rvert}$是类型的总数。 每个类型标签$t_i$是从根节点到终端节点的路径（例如，artist  表示为/ person / artist），并且终端节点可以是叶节点或非叶节点。 在本文中，我们使用终端类型作为先前研究中设定的预测目标[Xu and Barbosa，2018; Chen et al.，2019]。 在本文的大部分内容中，为简单起见，我们使用类型来表示终端类型。

由远距离监督构建的训练语料库D由三元组形式为（$m_i,c_i,y_i$），$i=1,2,···N$，其中$y_i \in \mathbb R^t$是标记向量（也表示为噪声标记，因为它可能包含不合适的类型）。 对于每个训练样本，我们将文本句子表示为单词序列$c_i = w_1,w_2,·· w_n$和实体提及$m_i = m_1,m_2,·· w_l$作为文本句子的连续子序列。 给定提及--上下文输入对，FET任务旨在根据预定义的候选集$\Gamma$预测最合适的类型$y_i^*$。

# 2.2 Feature Encoder  





# References



[^4 ]: [Abhishek et al., 2017] Abhishek. Fine-grained entity type classification by jointly learning representations and label embeddings. In EACL, pages 797–807, 2017.
[^5]:[Chen et al., 2019] Bo Chen.Improving distantly-supervised entity typing with compact latent space clustering. In NAACL-HLT, 2019.
[Dong et al., 2014] Xin Dong, Evgeniy Gabrilovich,Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. Knowledge vault: a web-scale approach to probabilistic knowledge fusion. In KDD, pages 601–610, 2014.
[Dong et al., 2015] Li Dong, Furu Wei, Hong Sun, Min gZhou, and Ke Xu. A hybrid neural model for type classification of entity mentions. In IJCAI, pages 1243–1249,2015.

[Gillick et al., 2014] Dan Gillick, Nevena Lazic, KuzmanGanchev, Jesse Kirchner, and David Huynh. Contextdependent fine-grained entity type tagging. arXiv preprintarXiv:1412.1820, 2014.
[Graves and Schmidhuber, 2005] Alex Graves and Jurgen ¨Schmidhuber. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural networks, 18(5-6):602–610, 2005.
[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. ¨ Neural computation, 9(8):1735–1780, 1997.
[Hu et al., 2017] Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete representations via information maximizing selfaugmented training. In ICML, pages 1558–1567, 2017.
[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[Lin and Ji, 2019] Ying Lin and Heng Ji. An attentive finegrained entity typing model with latent type representation. In EMNLP-IJCNLP, pages 6196–6201, 2019.
[Ling and Weld, 2012] Xiao Ling and Daniel S. Weld. Finegrained entity recognition. In AAAI, 2012.
[Liu et al., 2014] Yang Liu, Kang Liu, Liheng Xu, and Jun Zhao. Exploring fine-grained entity type constraints for
distantly supervised relation extraction. In COLING, pages 2107–2116, 2014.
[Ma et al., 2016] Yukun Ma, Erik Cambria, and Sa Gao. Label embedding for zero-shot fine-grained named entity
typing. In COLING, pages 171–180, 2016.

[^2]:[Mintz et al., 2009] Mike Mintz, Steven Bills, Rion Snow,and Daniel Jurafsky. Distant supervision for relation extraction without labeled data. In ACL-IJCNLP, pages 1003–1011, 2009.



[^1 ]:[Onoe and Durrett, 2019a] Yasumasa Onoe and Greg Durrett. Fine-grained entity typing for domain independent entity linking. arXiv preprint arXiv:1909.05780, 2019.
[Onoe and Durrett, 2019b] Yasumasa Onoe and Greg Durrett. Learning to denoise distantly-labeled data for entity typing. In NAACL-HLT, pages 2407–2417, 2019.



[^3]:[Ren et al., 2016a] Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng Ji, and Jiawei Han. AFET: automatic finegrained entity typing by hierarchical partial-label embedding. In EMNLP, pages 1369–1378, 2016. Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-20) 814
[Ren et al., 2016b] Xiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, and Jiawei Han. Label noise reduction in entity typing by heterogeneous partial-label embedding. In KDD, pages 1825–1834, 2016.
[ ^6  ]:  [Shimaoka et al., 2016] Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. An attentive neural
architecture for fine-grained entity type classification. In AKBC@NAACL-HLT, pages 69–74, 2016.

[Wang and Wu, 2019] Guo-Hua Wang and Jianxin Wu. Repetitive reprediction deep decipher for semi-supervised learning. arXiv preprint arXiv:1908.04345, 2019.
[Weischedel and Brunstein, 2005] Ralph Weischedel and Ada Brunstein. Bbn pronoun coreference and entity type
corpus. Linguistic Data Consortium, Philadelphia, 112, 2005.
[Weischedel et al., 2013] Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance
Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. Ontonotes release 5.0
ldc2013t19. Linguistic Data Consortium, Philadelphia, PA, 23, 2013.
[^8 ]:[Wu et al., 2019] Junshuang Wu, Richong Zhang, Yongyi Mao, Hongyu Guo, and Jinpeng Huai. Modeling noisy
hierarachical types in fine-grained entity typing: A contentbased weighting approach. In IJCAI, pages 5264–5270,
2019.
[Xin et al., 2018] Ji Xin, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Improving neural fine-grained entity typing with knowledge attention. In AAAI, 2018.
[Xu and Barbosa, 2018] Peng Xu and Denilson Barbosa. Neural fine-grained entity type classification with hierarchy-aware loss. In NAACL-HLT, pages 16–25,

[^7]: [Yogatama et al., 2015] Dani Yogatama, Daniel Gillick, andNevena Lazic. Embedding methods for fine grained entity type classification. In ACL, pages 291–296, 2015.
[Zeng et al., 2014] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classification via convolutional deep neural network. In COLING, pages 2335–2344, 2014.
[Zhou et al., 2016] Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, and Bo Xu. Attentionbased bidirectional long short-term memory networks for relation classification. In ACL, 2016.   

